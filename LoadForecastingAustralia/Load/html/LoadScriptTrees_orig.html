
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   <!--
This HTML is auto-generated from an M-file.
To make changes, update the M-file and republish this document.
      --><title>Load Forecasting using Bagged Regression Trees</title><meta name="generator" content="MATLAB 7.10"><meta name="date" content="2010-08-25"><meta name="m-file" content="LoadScriptTrees_orig"><style type="text/css">

body {
  background-color: white;
  margin:10px;
}

h1 {
  color: #990000; 
  font-size: x-large;
}

h2 {
  color: #990000;
  font-size: medium;
}

/* Make the text shrink to fit narrow windows, but not stretch too far in 
wide windows. */ 
p,h1,h2,div.content div {
  max-width: 600px;
  /* Hack for IE6 */
  width: auto !important; width: 600px;
}

pre.codeinput {
  background: #EEEEEE;
  padding: 10px;
}
@media print {
  pre.codeinput {word-wrap:break-word; width:100%;}
} 

span.keyword {color: #0000FF}
span.comment {color: #228B22}
span.string {color: #A020F0}
span.untermstring {color: #B20000}
span.syscmd {color: #B28C00}

pre.codeoutput {
  color: #666666;
  padding: 10px;
}

pre.error {
  color: red;
}

p.footer {
  text-align: right;
  font-size: xx-small;
  font-weight: lighter;
  font-style: italic;
  color: gray;
}

  </style></head><body><div class="content"><h1>Load Forecasting using Bagged Regression Trees</h1><!--introduction--><p>This example demonstrates an alternate model for building relationships between historical weather and load data to build and test a short term load forecasting. The model used is a set of aggregated Regression Trees.</p><!--/introduction--><h2>Contents</h2><div><ul><li><a href="#1">Import Data</a></li><li><a href="#3">Generate Predictor Matrix</a></li><li><a href="#4">Split the dataset to create a Training and Test set</a></li><li><a href="#5">Build the Bootstrap Aggregated Regression Trees</a></li><li><a href="#6">Determine Feature Importance</a></li><li><a href="#7">Build the Final Model</a></li><li><a href="#8">Save Trained Model</a></li><li><a href="#9">Test Results</a></li><li><a href="#10">Compute Prediction</a></li><li><a href="#11">Compare Forecasted Load and Actual Load</a></li><li><a href="#12">Compute Model Forecast Metrics</a></li></ul></div><h2>Import Data<a name="1"></a></h2><p>The data set used is a table of historical hourly loads and temperature observations from the New England ISO for the years 2004 to 2008. The weather information includes the dry bulb temperature and the dew point. This data set is imported from an Access database using the auto-generated function <i>fetchDBLoadData</i>. This function was generated by the Visual Query Builder in Database Toolbox and then modified.</p><pre class="codeinput">data = fetchDBLoadData(<span class="string">'2004-01-01'</span>, <span class="string">'2008-12-31'</span>);
</pre><p>Import list of holidays</p><pre class="codeinput">[num, text] = xlsread(<span class="string">'..\Data\Holidays.xls'</span>);
holidays = text(2:end,1);
</pre><h2>Generate Predictor Matrix<a name="3"></a></h2><p>The function <b>genPredictors</b> generates the predictor variables used as inputs for the model. For short-term forecasting these include</p><div><ul><li>Dry bulb temperature</li><li>Dew point</li><li>Hour of day</li><li>Day of the week</li><li>A flag indicating if it is a holiday/weekend</li><li>Previous day's average load</li><li>Load from the same hour the previous day</li><li>Load from the same hour and same day from the previous week</li></ul></div><p>If the goal is medium-term or long-term load forecasting, only the inputs hour of day, day of week, time of year and holidays can be used deterministically. The weather/load information would need to be specified as an average or a distribution</p><pre class="codeinput"><span class="comment">% Select forecast horizon</span>
term = <span class="string">'short'</span>;

[X, dates, labels] = genPredictors(data, term, holidays);
</pre><h2>Split the dataset to create a Training and Test set<a name="4"></a></h2><p>The dataset is divided into two sets, a <i>training</i> set which includes data from 2004 to 2007 and a <i>test</i> set with data from 2008. The training set is used for building the model (estimating its parameters). The test set is used only for forecasting to test the performance of the model on out-of-sample data.</p><pre class="codeinput"><span class="comment">% Create training set</span>
trainInd = data.NumDate &lt; datenum(<span class="string">'2008-01-01'</span>);
trainX = X(trainInd,:);
trainY = data.SYSLoad(trainInd);

<span class="comment">% Create test set and save for later</span>
testInd = data.NumDate &gt;= datenum(<span class="string">'2008-01-01'</span>);
testX = X(testInd,:);
testY = data.SYSLoad(testInd);
testDates = dates(testInd);

save <span class="string">Data\testSet</span> <span class="string">testDates</span> <span class="string">testX</span> <span class="string">testY</span>
clear <span class="string">X</span> <span class="string">data</span> <span class="string">trainInd</span> <span class="string">testInd</span> <span class="string">term</span> <span class="string">holidays</span> <span class="string">dates</span> <span class="string">ans</span>
</pre><h2>Build the Bootstrap Aggregated Regression Trees<a name="5"></a></h2><p>The function TreeBagger is used to build the model, ie. a set of regression trees each with a different set of rules for performing the non-linear regression. We initially start by building an aggregate of 20 such trees, with a minimum leaf size of 40. The larger the leaf size the smaller the tree. This provides a control for overfitting and performance.</p><pre class="codeinput">model = TreeBagger(20, trainX, trainY, <span class="string">'method'</span>, <span class="string">'regression'</span>, <span class="string">'minleaf'</span>, 40)

simpleTree = prune(model.Trees{1}, 500);
<span class="comment">% view(simpleTree, 'names', labels);</span>
</pre><pre class="codeoutput">model = 
Ensemble with 20 bagged decision trees:
               Training X:            [35064x8]
               Training Y:            [35064x1]
                   Method:           regression
                    Nvars:                    8
             NVarToSample:                    3
                  MinLeaf:                   40
                    FBoot:                    1
    SampleWithReplacement:                    1
     ComputeOOBPrediction:                    0
         ComputeOOBVarImp:                    0
                Proximity:                   []
</pre><h2>Determine Feature Importance<a name="6"></a></h2><p>Of each of the predictors, which ones provide the most predictive power? Turning on the <i>oobVarImp</i> parameter shows you out-of-bag estimates of this relative feature (input) importance.</p><pre class="codeinput">model = TreeBagger(20, trainX, trainY, <span class="string">'method'</span>, <span class="string">'regression'</span>, <span class="keyword">...</span>
                   <span class="string">'oobvarimp'</span>, <span class="string">'on'</span>, <span class="string">'minleaf'</span>, 30);

figure(2);
barh(model.OOBPermutedVarDeltaError);
ylabel(<span class="string">'Feature'</span>);
xlabel(<span class="string">'Out-of-bag feature importance'</span>);
title(<span class="string">'Feature importance results'</span>);
set(gca, <span class="string">'YTickLabel'</span>, labels)
</pre><img vspace="5" hspace="5" src="LoadScriptTrees_orig_01.png" alt=""> <h2>Build the Final Model<a name="7"></a></h2><p>Given our analysis of parameters, we may wish to now build the final model with 20 trees, a leaf size of 20 and all of the features</p><pre class="codeinput">model = TreeBagger(20, trainX, trainY, <span class="string">'method'</span>, <span class="string">'regression'</span>, <span class="string">'minleaf'</span>, 20);
</pre><h2>Save Trained Model<a name="8"></a></h2><p>We can compact the model (to remove any stored training data) and save for later reuse</p><pre class="codeinput">model = compact(model);
save <span class="string">Models\TreeModel</span> <span class="string">model</span>
</pre><h2>Test Results<a name="9"></a></h2><p>Load in the model and test data and run the treeBagger forecaster and compare to actual load.</p><pre class="codeinput">clear
cd <span class="string">Models</span>
load <span class="string">Models\TreeModel</span>
cd <span class="string">..</span>
cd <span class="string">Data</span>
load <span class="string">Data\testSet</span>
cd <span class="string">..</span>
</pre><h2>Compute Prediction<a name="10"></a></h2><p>Predict the load for 2008 using the model trained on load data from 2007 and before.</p><pre class="codeinput">forecastLoad = predict(model, testX);
</pre><h2>Compare Forecasted Load and Actual Load<a name="11"></a></h2><p>Create a plot to compare the actual load and the predicted load as well as the forecast error.</p><pre class="codeinput">ax1 = subplot(2,1,1);
plot(testDates, [testY forecastLoad]);
ylabel(<span class="string">'Load'</span>); legend({<span class="string">'Actual'</span>, <span class="string">'Forecast'</span>}); legend(<span class="string">'boxoff'</span>)
ax2 = subplot(2,1,2);
plot(testDates, testY-forecastLoad);
xlabel(<span class="string">'Date'</span>); ylabel(<span class="string">'Error (MWh)'</span>);
linkaxes([ax1 ax2], <span class="string">'x'</span>);
dynamicDateTicks([ax1 ax2], <span class="string">'linked'</span>)
</pre><img vspace="5" hspace="5" src="LoadScriptTrees_orig_02.png" alt=""> <h2>Compute Model Forecast Metrics<a name="12"></a></h2><p>In addition to the visualization we can quantify the performance of the forecaster using metrics such as mean average error (MAE), mean average percent error (MAPE) and daily peak forecast error.</p><pre class="codeinput">err = testY-forecastLoad;
errpct = abs(err)./testY*100;

fL = reshape(forecastLoad, 24, length(forecastLoad)/24)';
tY = reshape(testY, 24, length(testY)/24)';
peakerrpct = abs(max(tY,[],2) - max(fL,[],2))./max(tY,[],2) * 100;

fprintf(<span class="string">'Mean Average Percent Error (MAPE): %0.2f%% \nMean Average Error (MAE): %0.2f MWh\nDaily Peak MAPE: %0.2f%%\n'</span>,<span class="keyword">...</span>
    mean(errpct(~isinf(errpct))), mean(abs(err)), mean(peakerrpct))
</pre><pre class="codeoutput">Mean Average Percent Error (MAPE): 2.19% 
Mean Average Error (MAE): 330.48 MWh
Daily Peak MAPE: 2.25%
</pre><p class="footer"><br>
      Published with MATLAB&reg; 7.10<br></p></div><!--
##### SOURCE BEGIN #####
%% Load Forecasting using Bagged Regression Trees
% This example demonstrates an alternate model for building relationships
% between historical weather and load data to build and test a short term
% load forecasting. The model used is a set of aggregated Regression Trees.

%% Import Data
% The data set used is a table of historical hourly loads and temperature
% observations from the New England ISO for the years 2004 to 2008. The
% weather information includes the dry bulb temperature and the dew point.
% This data set is imported from an Access database using the
% auto-generated function _fetchDBLoadData_. This function was generated by
% the Visual Query Builder in Database Toolbox and then modified.

data = fetchDBLoadData('2004-01-01', '2008-12-31');

%%
% Import list of holidays
[num, text] = xlsread('..\Data\Holidays.xls');
holidays = text(2:end,1);


%% Generate Predictor Matrix
% The function *genPredictors* generates the predictor variables used as
% inputs for the model. For short-term forecasting these include
% 
% * Dry bulb temperature
% * Dew point
% * Hour of day
% * Day of the week
% * A flag indicating if it is a holiday/weekend
% * Previous day's average load
% * Load from the same hour the previous day
% * Load from the same hour and same day from the previous week
%
% If the goal is medium-term or long-term load forecasting, only the inputs
% hour of day, day of week, time of year and holidays can be used
% deterministically. The weather/load information would need to be
% specified as an average or a distribution

% Select forecast horizon
term = 'short';

[X, dates, labels] = genPredictors(data, term, holidays);

%% Split the dataset to create a Training and Test set
% The dataset is divided into two sets, a _training_ set which includes 
% data from 2004 to 2007 and a _test_ set with data from 2008. The training
% set is used for building the model (estimating its parameters). The test
% set is used only for forecasting to test the performance of the model on 
% out-of-sample data. 

% Create training set
trainInd = data.NumDate < datenum('2008-01-01');
trainX = X(trainInd,:);
trainY = data.SYSLoad(trainInd);

% Create test set and save for later
testInd = data.NumDate >= datenum('2008-01-01');
testX = X(testInd,:);
testY = data.SYSLoad(testInd);
testDates = dates(testInd);

save Data\testSet testDates testX testY
clear X data trainInd testInd term holidays dates ans
%% Build the Bootstrap Aggregated Regression Trees
% The function TreeBagger is used to build the model, ie. a set of regression
% trees each with a different set of rules for performing the non-linear regression.
% We initially start by building an aggregate of 20 such trees, with a
% minimum leaf size of 40. The larger the leaf size the smaller the tree.
% This provides a control for overfitting and performance.

model = TreeBagger(20, trainX, trainY, 'method', 'regression', 'minleaf', 40)

simpleTree = prune(model.Trees{1}, 500);
% view(simpleTree, 'names', labels);


%% Determine Feature Importance
% Of each of the predictors, which ones provide the most predictive power?
% Turning on the _oobVarImp_ parameter shows you out-of-bag estimates of this
% relative feature (input) importance. 

model = TreeBagger(20, trainX, trainY, 'method', 'regression', ...
                   'oobvarimp', 'on', 'minleaf', 30);

figure(2);
barh(model.OOBPermutedVarDeltaError);
ylabel('Feature');
xlabel('Out-of-bag feature importance');
title('Feature importance results');
set(gca, 'YTickLabel', labels)

%% Build the Final Model
% Given our analysis of parameters, we may wish to now build the final
% model with 20 trees, a leaf size of 20 and all of the features

model = TreeBagger(20, trainX, trainY, 'method', 'regression', 'minleaf', 20);

%% Save Trained Model
% We can compact the model (to remove any stored training data) and save
% for later reuse

model = compact(model);
save Models\TreeModel model


%% Test Results
% Load in the model and test data and run the treeBagger forecaster and
% compare to actual load.

clear
cd Models
load Models\TreeModel
cd ..
cd Data
load Data\testSet
cd ..

%% Compute Prediction
% Predict the load for 2008 using the model trained on load data from 2007
% and before.
forecastLoad = predict(model, testX);

%% Compare Forecasted Load and Actual Load
% Create a plot to compare the actual load and the predicted load as well
% as the forecast error.

ax1 = subplot(2,1,1);
plot(testDates, [testY forecastLoad]);
ylabel('Load'); legend({'Actual', 'Forecast'}); legend('boxoff')
ax2 = subplot(2,1,2);
plot(testDates, testY-forecastLoad);
xlabel('Date'); ylabel('Error (MWh)');
linkaxes([ax1 ax2], 'x');
dynamicDateTicks([ax1 ax2], 'linked')

%% Compute Model Forecast Metrics
% In addition to the visualization we can quantify the performance of the
% forecaster using metrics such as mean average error (MAE), mean average
% percent error (MAPE) and daily peak forecast error.

err = testY-forecastLoad;
errpct = abs(err)./testY*100;

fL = reshape(forecastLoad, 24, length(forecastLoad)/24)';
tY = reshape(testY, 24, length(testY)/24)';
peakerrpct = abs(max(tY,[],2) - max(fL,[],2))./max(tY,[],2) * 100;

fprintf('Mean Average Percent Error (MAPE): %0.2f%% \nMean Average Error (MAE): %0.2f MWh\nDaily Peak MAPE: %0.2f%%\n',...
    mean(errpct(~isinf(errpct))), mean(abs(err)), mean(peakerrpct))
##### SOURCE END #####
--></body></html>